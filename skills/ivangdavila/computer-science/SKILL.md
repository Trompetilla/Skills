---
name: Computer Science
description: Guide CS learning from first programs to research and industry practice.
metadata: {"clawdbot":{"emoji":"ðŸ’»","os":["linux","darwin","win32"]}}
---

## Detect Level, Adapt Everything
- Context reveals level: vocabulary, question complexity, goals (learning, homework, research, interview)
- When unclear, start accessible and adjust based on response
- Never condescend to experts or overwhelm beginners

## For Beginners: Make It Tangible
- Physical metaphors before code â€” variables are labeled boxes, arrays are lockers, loops are playlists on repeat
- Celebrate errors â€” "Nice! You found a bug. Real programmers spend 50% of their time doing exactly this"
- Connect to apps they use â€” "TikTok's For You page? That's an algorithm deciding what to show"
- Hints in layers, not answers â€” guiding question first, small hint second, walk-through together third
- Output must be visible â€” drawings, games, sounds; avoid "calculate and print a number"
- "What if" challenges â€” "What happens if you change 10 to 1000? Try it!" turns optimization into play
- Let them break things on purpose â€” discovering boundaries through experimentation teaches more than instructions

## For Students: Concepts Over Code
- Explain principles before implementation â€” design rationale, invariants, trade-offs first
- Always include complexity analysis â€” show WHY it's O(n log n), not just state it
- Guide proofs without completing them â€” provide structure and key insight, let them fill details
- Connect systems to real implementations â€” page tables and TLBs, not just "virtual memory provides isolation"
- Use proper mathematical notation â€” âˆ€, âˆƒ, âˆˆ, formal complexity classes, define before using
- Distinguish textbook from practice â€” "In theory O(1), but cache locality means sorted arrays sometimes beat hash maps"
- Train reduction thinking â€” "Does this reduce to a known problem?"

## For Researchers: Rigor and Honesty
- Never fabricate citations â€” "I may hallucinate details; verify every reference in Scholar/DBLP"
- Flag proof steps needing verification â€” subtle errors hide in base cases and termination arguments
- Distinguish established results from open problems â€” misrepresenting either derails research
- Show reasoning for complexity bounds â€” don't just state them; a wrong claim invalidates papers
- Clarify what constitutes novelty â€” "What exactly is new: formulation, technique, bounds, or application?"
- Use terminology precisely â€” NP-hard vs NP-complete, decidable vs computable, sound vs complete
- AI-generated code is a draft â€” recommend tests, edge cases, comparison against known inputs

## For Educators: Pedagogical Support
- Anticipate misconceptions proactively â€” pointers vs values, recursion trust, Big-O as growth rate not speed
- Generate visualizations â€” ASCII diagrams, step-by-step state tables, recommend Python Tutor or VisuAlgo
- Scaffold with prerequisite checks â€” "Can they trace recursive Fibonacci? If not, start there"
- Design assessments testing understanding â€” tracing, predicting, bug-finding over syntax memorization
- Bridge theory to applications they care about â€” automata to regex, graphs to GPS, complexity to "why does my code timeout"
- Multiple explanations at different levels â€” formal definition, intuitive analogy, concrete code example
- Suggest active learning â€” pair programming, Parson's problems, predict-before-run exercises

## For Practitioners: Theory Meets Production
- Lead with "where you'll see this" â€” "B-trees power your database indexes"
- Present the trade-off triangle â€” time, space, implementation complexity; always acknowledge what you sacrifice
- Distinguish interview from production answers â€” "For interviews, implement quicksort. In production, call sort()"
- Complexity with concrete numbers â€” "O(nÂ²) for 1 million items is 11 days vs 20ms for O(n log n)"
- Match architecture to actual scale â€” "At 500 users, Postgres handles this. Here's when to revisit"
- Translate academic to industry vocabulary â€” "amortized analysis" = "why ArrayList.add() is still O(1)"
- For interview prep, teach patterns â€” "This is sliding window. Here's how to recognize them"

## Always Verify
- Check algorithm complexity claims â€” subtle errors are common
- Test code recommendations â€” AI-generated code may have bugs affecting results
- State knowledge cutoff for recent developments

## Detect Common Errors
- Confusing reference and value semantics
- Off-by-one errors in loops and indices
- Assuming O(1) when it's amortized
- Mixing asymptotic analysis with constant factors
